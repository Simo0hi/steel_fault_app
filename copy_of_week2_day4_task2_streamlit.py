# -*- coding: utf-8 -*-
"""Copy of Week2_Day4_Task2_Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c2yVGAIY5ZuMTkWDi7_ooPz01xNDvmzR

Create a UI for your ML models using Gradio and Streamlit, each in a separate project then upload your project in github

Gardio resources:

Vedio--> https://youtu.be/wruyZWre2sM?si=ZJWu_Cr9uxPBXLFR

Tutorial--> https://www.machinelearningnuggets.com/gradio-tutorial/

Streamlit resources:

Vedio--> https://youtu.be/Klqn--Mu2pE?si=JPAMk8v1x0AJsouJ

Tutorial--> https://www.datacamp.com/tutorial/streamlit
"""

# Download the dataset
!kaggle datasets download -d shrutimechlearn/steel-plate-fault

# Unzip the dataset
!unzip steel-plate-fault.zip

import pandas as pd
df = pd.read_csv('faults.csv')

# prompt: preprocessing the data

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming 'df' is your DataFrame as defined in the preceding code

# Handle missing values (if any)
df = df.dropna()  # Or use imputation techniques if appropriate

# Separate features and target variable
X = df.drop('target', axis=1)  # Replace 'TypeOfSteel_A300' with your target column
y = df['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# 2. Support Vector Machine (SVM)
svm_model = SVC()
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

# 3. Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ... (Your existing code from the preceding code section)

# ... (Your model training and evaluation code)

# Display confusion matrix for each model
cm_lr = confusion_matrix(y_test, lr_predictions)
cm_svm = confusion_matrix(y_test, svm_predictions)
cm_rf = confusion_matrix(y_test, rf_predictions)

plt.figure(figsize=(18, 6))

plt.subplot(131)
sns.heatmap(cm_lr, annot=True, fmt='d', cmap="Blues")
plt.title("Logistic Regression Confusion Matrix")

plt.subplot(132)
sns.heatmap(cm_svm, annot=True, fmt='d', cmap="Blues")
plt.title("SVM Confusion Matrix")

plt.subplot(133)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap="Blues")
plt.title("Random Forest Confusion Matrix")

plt.tight_layout()
plt.show()

df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile steel_fault_app.py

!wget -q -O - ipv4.icanhazip.com

!streamlit run steel_fault_app.py & npx localtunnel --port 8501

!pip install streamlit
# Create a new Python file named "steel_fault_app.py" and paste the following code:

import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Load the dataset (replace 'faults.csv' with the actual path)
df = pd.read_csv('faults.csv')

# Handle missing values (if any)
df = df.dropna()

# Separate features and target variable
X = df.drop('target', axis=1)
y = df['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the models
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

svm_model = SVC()
svm_model.fit(X_train, y_train)

rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Streamlit app
st.title("Steel Plate Fault Prediction")

# Input fields for features
st.sidebar.header("Input Parameters")
input_features = {}
for feature in X.columns:
    input_features[feature] = st.sidebar.number_input(feature, value=X[feature].mean())

# Create a DataFrame from input features
input_df = pd.DataFrame([input_features])

# Scale input features
input_df_scaled = scaler.transform(input_df)

# Model selection
model_choice = st.sidebar.selectbox("Select Model", ["Logistic Regression", "SVM", "Random Forest"])

# Prediction
if st.button("Predict"):
    if model_choice == "Logistic Regression":
        prediction = lr_model.predict(input_df_scaled)[0]
    elif model_choice == "SVM":
        prediction = svm_model.predict(input_df_scaled)[0]
    else:
        prediction = rf_model.predict(input_df_scaled)[0]

    st.success(f"Predicted Fault Type: {prediction}")

# Run the app:
# 1. Save the code as "steel_fault_app.py"
# 2. Open your terminal and navigate to the directory containing the file
# 3. Run the command: streamlit run steel_fault_app.py

!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py

