# -*- coding: utf-8 -*-
"""Copy of Week2_Day4_Task2_Gardio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YxTTP-hrrVEyzVsPE-zbPcGJd8T4u7K9

Create a UI for your ML models using Gradio and Streamlit, each in a separate project then upload your project in github

Gardio resources:

Vedio--> https://youtu.be/wruyZWre2sM?si=ZJWu_Cr9uxPBXLFR

Tutorial--> https://www.machinelearningnuggets.com/gradio-tutorial/

Streamlit resources:

Vedio--> https://youtu.be/Klqn--Mu2pE?si=JPAMk8v1x0AJsouJ

Tutorial--> https://www.datacamp.com/tutorial/streamlit
"""

# Download the dataset
!kaggle datasets download -d mirichoi0218/insurance

# Unzip the dataset
!unzip insurance.zip

import pandas as pd
df_insurance = pd.read_csv('insurance.csv')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ... (Your existing code from the preceding code section)

# Identify numerical columns
numerical_cols = ['age', 'bmi', 'children', 'charges']

# Calculate IQR for each numerical column
Q1 = df_insurance[numerical_cols].quantile(0.25)
Q3 = df_insurance[numerical_cols].quantile(0.75)
IQR = Q3 - Q1

# Define outlier bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter outliers
df_insurance_no_outliers = df_insurance[~((df_insurance[numerical_cols] < lower_bound) | (df_insurance[numerical_cols] > upper_bound)).any(axis=1)]

# Display the shape of the DataFrame before and after removing outliers
print("Shape before removing outliers:", df_insurance.shape)
print("Shape after removing outliers:", df_insurance_no_outliers.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Handle missing values (if any)
df_insurance = df_insurance.dropna()  # Or use imputation techniques if appropriate

# Separate features and target variable
X = df_insurance.drop('charges', axis=1)
y = df_insurance['charges']

# Identify numerical and categorical features
numerical_features = ['age', 'bmi', 'children']
categorical_features = ['sex', 'smoker', 'region']

# Create transformers for numerical and categorical features
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Apply preprocessing to the features
X_processed = preprocessor.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ... (Your existing code from the preceding code section)
# 1. Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)
lr_mse = mean_squared_error(y_test, lr_predictions)
print("Linear Regression MSE:", lr_mse)

# 2. Decision Tree Regressor
dt_model = DecisionTreeRegressor()
dt_model.fit(X_train, y_train)
dt_predictions = dt_model.predict(X_test)
dt_mse = mean_squared_error(y_test, dt_predictions)
print("Decision Tree Regressor MSE:", dt_mse)

# 3. Random Forest Regressor
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)
print("Random Forest Regressor MSE:", rf_mse)

# Import libraries
import gradio as gr
from gradio import components

# Assuming preprocessor and lr_model are defined elsewhere

def predict_charges(age, sex, bmi, children, smoker, region):
  # Create input data as DataFrame
  input_data = pd.DataFrame({
      "age": [age],
      "sex": [sex],
      "bmi": [bmi],
      "children": [children],
      "smoker": [smoker],
      "region": [region]
  })

  # Preprocess data (assuming preprocessor is available)
  input_processed = preprocessor.transform(input_data)

  # Make prediction using lr_model
  prediction = lr_model.predict(input_processed)[0]

  return prediction

inputs = [
  components.Number(label="Age"),
  components.Dropdown(choices=["male", "female"], label="Sex"),
  components.Number(label="BMI"),
  components.Number(label="Children"),
  components.Dropdown(choices=["yes", "no"], label="Smoker"),
  components.Dropdown(choices=["southwest", "southeast", "northwest", "northeast"], label="Region")
]
output = components.Textbox(label="Predicted Charges")

# Launch the Gradio interface
gr_interface = gr.Interface(fn=predict_charges, inputs=inputs, outputs=output)
gr_interface.launch()

